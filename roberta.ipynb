{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac68c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from wordcloud import wordcloud\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58871003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654da52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(batched_text):\n",
    "    return tokenizer(batched_text['Description'], padding = True, truncation=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97678e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=train_test_split(res,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd76509",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenization(train_data)\n",
    "test_data = tokenization(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9680b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, test_data = datasets.load_dataset(res, split =['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230b5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a036a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "# EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e68ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.Description\n",
    "        self.targets = self.data.Code\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682a62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "res['Code']= label_encoder.fit_transform(res['Code'])\n",
    "\n",
    "res['Code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de859d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_1=res[['Description','Code_1']]\n",
    "new_df=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_data=new_df.sample(frac=train_size,random_state=200)\n",
    "test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = SentimentData(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8365bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7414df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 24)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b101f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af97200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693021ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931eca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14052127",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e941be",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ed2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50299633",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['Code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfcd97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e673ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eae3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c10502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 80000\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 48 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vocab_size = 80000\n",
    "\n",
    "tokenizer = Tokenizer(lower=False, num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_enc = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_enc = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 10000\n",
    "\n",
    "X_train_pd = pad_sequences(X_train_enc, padding='post', maxlen=max_len)\n",
    "X_test_pd = pad_sequences(X_test_enc, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b99565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 10000, 16)         1280000   \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 10000, 128)       41472     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 9993, 64)          65600     \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 4996, 64)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 4996, 128)        66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 4991, 64)          49216     \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 2495, 64)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 2495, 128)        66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 2493, 64)          24640     \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 1246, 64)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 79744)             0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 79744)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               10207360  \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,808,835\n",
      "Trainable params: 11,808,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "seed_value = 1337\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "\n",
    "\n",
    "# hyper parameters\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 256\n",
    "embedding_dim = 16\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    L.Embedding(vocab_size, embedding_dim, input_length=X_train_pd.shape[1]),\n",
    "    L.Bidirectional(L.LSTM(64,return_sequences=True)),\n",
    "    L.Conv1D(64,8),\n",
    "    L.MaxPool1D(),\n",
    "    L.Bidirectional(L.LSTM(64,return_sequences=True)),\n",
    "    L.Conv1D(64,6),\n",
    "    L.MaxPool1D(),\n",
    "    L.Bidirectional(L.LSTM(64,return_sequences=True)),\n",
    "    L.Conv1D(64,3),\n",
    "    L.MaxPool1D(),\n",
    "    #L.LSTM(64,return_sequences=True),\n",
    "    #L.Conv1D(64,2),\n",
    "    #L.GlobalMaxPooling1D(),\n",
    "    L.Flatten(),\n",
    "    L.Dropout(0.5),\n",
    "    L.Dense(128, activation=\"relu\"),\n",
    "    L.Dropout(0.5),\n",
    "    L.Dense(64, activation=\"relu\"),\n",
    "    L.Dropout(0.5),\n",
    "    L.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb44d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_pd, y_train, epochs=EPOCHS, validation_split=0.12, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb461ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    history.history, y=['loss', 'val_loss'],\n",
    "    labels={'index': 'epoch', 'value': 'loss'}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735fde7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X_test_pd[0:1000], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35275764",
   "metadata": {},
   "outputs": [],
   "source": [
    "90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156bd43",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b95c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "693ab1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = \"https://tfhub.dev/google/experts/bert/wiki_books/2\" # @param {type: \"string\"} [\"https://tfhub.dev/google/experts/bert/wiki_books/2\", \"https://tfhub.dev/google/experts/bert/wiki_books/mnli/2\", \"https://tfhub.dev/google/experts/bert/wiki_books/qnli/2\", \"https://tfhub.dev/google/experts/bert/wiki_books/qqp/2\", \"https://tfhub.dev/google/experts/bert/wiki_books/squad2/2\", \"https://tfhub.dev/google/experts/bert/wiki_books/sst2/2\",  \"https://tfhub.dev/google/experts/bert/pubmed/2\", \"https://tfhub.dev/google/experts/bert/pubmed/squad2/2\"]\n",
    "# Preprocessing must match the model, but all the above use the same.\n",
    "PREPROCESS_MODEL = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cebf04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6aabf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6005520f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "811da30531c61cad652a186487c6054dda06260fc7cfc286d1d5b859e7d68967"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
